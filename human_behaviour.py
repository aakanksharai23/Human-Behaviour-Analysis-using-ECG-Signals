# -*- coding: utf-8 -*-
"""Human behaviour.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12yTWuCfl4UvB8bTipb0KBTbgmkvp2O1b
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from keras import layers
import keras

# %matplotlib inline
tf.random.set_seed(42)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("gaurav2022/mobile-health")

print("Path to dataset files:", path)

import pandas as pd
import os

dataset_dir = '/root/.cache/kagglehub/datasets/gaurav2022/mobile-health/versions/1'

csv_files = [f for f in os.listdir(dataset_dir) if f.endswith('.csv')]

if csv_files:
    csv_file_path = os.path.join(dataset_dir, csv_files[0])

    df = pd.read_csv(csv_file_path)
    print(df)

df

from sklearn.utils import resample

df_majority = df[df.Activity==0]
df_minorities = df[df.Activity!=0]

df_majority_downsampled = resample(df_majority,n_samples=30000, random_state=42)
df = pd.concat([df_majority_downsampled, df_minorities])
df.Activity.value_counts()

df1 = df.copy()
for feature in df1.columns[:-2]:
  lower_range = np.quantile(df[feature],0.01)
  upper_range = np.quantile(df[feature],0.99)
  print(feature,'range:',lower_range,'to',upper_range)

  df1 = df1.drop(df1[(df1[feature]>upper_range) | (df1[feature]<lower_range)].index, axis=0)
  print('shape',df1.shape)

label_map = {
    0: 'Nothing',
    1: 'Standing still',
    2: 'Sitting and relaxing',
    3: 'Lying down',
    4: 'Walking',
    5: 'Climbing stairs',
    6: 'Waist bends forward',
    7: 'Frontal elevation of arms',
    8: 'Knees bending (crouching)',
    9: 'Cycling',
    10: 'Jogging',
    11: 'Running',
    12: 'Jump front & back'
}

#spliting data into train and test set
print(df1.shape)
train = df1[(df1['subject'] != 'subject10') & (df1['subject'] != 'subject9')]
test = df1.drop(train.index, axis=0)
train.shape,test.shape

X_train = train.drop(['Activity','subject'],axis=1)
y_train = train['Activity']
X_test = test.drop(['Activity','subject'],axis=1)
y_test = test['Activity']
X_train.shape,y_train.shape,X_test.shape,y_test.shape

from scipy import stats
import numpy as np

from scipy import stats
import numpy as np

def create_dataset(X, y, time_steps, step=1):
    Xs, ys = [], []
    for i in range(0, len(X) - time_steps, step):
        x = X.iloc[i:(i + time_steps)].values
        labels = y.iloc[i: i + time_steps]
        Xs.append(x)
        # Check if mode is a scalar, if so, access it directly
        mode_result = stats.mode(labels)
        ys.append(mode_result.mode[0] if isinstance(mode_result.mode, np.ndarray) else mode_result.mode)
    return np.array(Xs), np.array(ys).reshape(-1, 1)

TIME_STEPS = 100
STEP = 40

# Assuming X_train, y_train are your training data
X_train_seq, y_train_seq = create_dataset(X_train, y_train, TIME_STEPS, STEP)
X_test_seq, y_test_seq = create_dataset(X_test, y_test, TIME_STEPS, STEP)

# Now X_train_seq and y_train_seq are ready for your sequence model
print(X_train_seq.shape, y_train_seq.shape)

model = keras.Sequential()
model.add(layers.Input(shape=[100,12]))
model.add(layers.Conv1D(filters=32, kernel_size=3, padding="same"))
model.add(layers.BatchNormalization())
model.add(layers.ReLU())
model.add(layers.Conv1D(filters=64, kernel_size=3, padding="same"))
model.add(layers.BatchNormalization())
model.add(layers.ReLU())
model.add(layers.MaxPool1D(2))
model.add(layers.LSTM(64))
model.add(layers.Dense(units=128, activation='relu'))
model.add(layers.Dense(13, activation='softmax'))
model.summary()

model1 = keras.Sequential()
model1.add(layers.Input(shape=[100,12]))
model1.add(layers.Conv1D(filters=16, kernel_size=3, padding="same"))
model1.add(layers.BatchNormalization())
model1.add(layers.Dropout(0.2))

tf.keras.utils.plot_model(model1, show_shapes=True)

tf.keras.utils.plot_model(model, show_shapes=True)

callbacks = [keras.callbacks.ModelCheckpoint("mhealth_best.keras", save_best_only=True, monitor="val_loss"),
             keras.callbacks.EarlyStopping(monitor="val_loss", patience=50, verbose=1)]

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["sparse_categorical_accuracy"],)

model_history = model.fit(X_train_seq,y_train_seq, epochs= 50, batch_size = 1024, validation_data=(X_test_seq,y_test_seq), callbacks=callbacks)

train_loss = model_history.history['loss']
val_loss = model_history.history['val_loss']
train_accuracy = model_history.history['sparse_categorical_accuracy']
val_accuracy = model_history.history['val_sparse_categorical_accuracy']

plt.figure(figsize=(12,6))

plt.subplot(1,2,1)
plt.plot(train_loss, 'r', label='Training loss')
plt.plot(val_loss, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_accuracy, 'r', label='Training Accuracy')
plt.plot(val_accuracy, 'b', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model = keras.models.load_model('./mhealth_best.keras')

train_loss, train_acc = model.evaluate(X_train_seq,y_train_seq)
test_loss, test_acc = model.evaluate(X_test_seq,y_test_seq)

print("Train accuracy", round(train_acc*100, 2),'%')
print("Train loss", train_loss)
print("Test accuracy", round(test_acc*100, 2),'%')
print("Test loss", test_loss)

pred = model.predict(X_test_seq)
pred = np.argmax(pred, axis = 1)
pred = pred.reshape(-1,1)

pred.shape,y_test.shape

from sklearn.metrics import confusion_matrix, classification_report

print(classification_report(y_test_seq,pred))
print('*'*50)
print(confusion_matrix(y_test_seq,pred))

plt.figure(figsize=(12,8))
conf_matrix = confusion_matrix(y_test_seq,pred)
sns.heatmap(conf_matrix, xticklabels= label_map.values(), yticklabels= label_map.values(), annot=True, fmt="d")
plt.show()